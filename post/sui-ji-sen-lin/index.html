
<!DOCTYPE html>
<html>
  <head>
    
<meta charset="utf-8" >

<title>随机森林 | 书生海海</title>
<meta name="description" content="温故而知新">

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.min.css">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="shortcut icon" href="https://aprilspring.github.io/favicon.ico?v=1667029754307">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
<link rel="stylesheet" href="https://aprilspring.github.io/styles/main.css">


  
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css" />
  

  


<script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>
<script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.5.1/build/highlight.min.js"></script>



  </head>
  <body>
    <div id="app" class="main">
      <div class="site-header-container">
  <div class="site-header">
    <div class="left">
      <a href="https://aprilspring.github.io">
        <img class="avatar" src="https://aprilspring.github.io/images/avatar.png?v=1667029754307" alt="" width="32px" height="32px">
      </a>
      <a href="https://aprilspring.github.io">
        <h1 class="site-title">书生海海</h1>
      </a>
    </div>
    <div class="right">
      <transition name="fade">
        <i class="icon" :class="{ 'icon-close-outline': menuVisible, 'icon-menu-outline': !menuVisible }" @click="menuVisible = !menuVisible"></i>
      </transition>
    </div>
  </div>
</div>

<transition name="fade">
  <div class="menu-container" style="display: none;" v-show="menuVisible">
    <div class="menu-list">
      
        
          <a href="/" class="menu purple-link">
            首页
          </a>
        
      
        
          <a href="/archives" class="menu purple-link">
            归档
          </a>
        
      
        
          <a href="/tags" class="menu purple-link">
            标签
          </a>
        
      
        
          <a href="/post/about" class="menu purple-link">
            关于
          </a>
        
      
    </div>
  </div>
</transition>


      <div class="content-container">
        <div class="post-detail">
          
          <h2 class="post-title">随机森林</h2>
          <div class="post-info post-detail-info">
            <span><i class="icon-calendar-outline"></i> 2019-07-22</span>
            
              <span>
                <i class="icon-pricetags-outline"></i>
                
                  <a href="https://aprilspring.github.io/tag/Fgq3FAATx/">
                    MachineLearning
                    
                  </a>
                
              </span>
            
          </div>
          <div class="post-content" v-pre>
            <p>随机森林是有很多随机得决策树构成，它们之间没有关联。得到RF以后，在预测时分别对每一个决策树进行判断，最后使用Bagging的思想进行结果的输出（也就是投票的思想）。</p>
<h2 id="学习过程简介">学习过程简介</h2>
<p>现在有N个训练样本，每个样本的特征为M个，需要建K颗树</p>
<p>从N个训练样本中有放回的取N个样本作为一组训练集（其余未取到的样本作为预测分类，评估其误差）</p>
<p>从M个特征中取m个特征左右子集特征(m&lt;&lt;M)</p>
<p>对采样的数据使用完全分裂的方式来建立决策树，这样的决策树每个节点要么无法分裂，要么所有的样本都指向同一个分类</p>
<p>重复2的过程K次，即可建立森林</p>
<!--more-->
<h2 id="预测过程">预测过程</h2>
<p>1）将预测样本输入到K颗树分别进行预测</p>
<p>2）如果是分类问题，直接使用投票的方式选择分类频次最高的类别</p>
<p>3）如果是回归问题，使用分类之后的均值作为结果</p>
<h2 id="参数说明">参数说明</h2>
<p>1）这里的一般取m=sqrt(M)</p>
<p>2）关于树的个数K，一般都需要成百上千，但是也有具体的样本有关（比如特征数量）</p>
<p>3）树的最大深度，（太深可能可能导致过拟合？？）</p>
<p>4）节点上的最小样本数、最小信息增益</p>
<figure data-type="image" tabindex="1"><img src="https://aprilspring.github.io/post-images/1667022117697.png" alt="" loading="lazy"></figure>
<h2 id="泛化误差估计">泛化误差估计</h2>
<p>使用oob（out-of-bag）进行泛化误差的估计，将各个树的未采样样本作为预测样本（大约有36.8%），使用已经建立好的森林对各个预测样本进行预测，预测完之后最后统计误判样本占总预测样本的比率作为RF的<strong>oob误分率</strong>。</p>
<h2 id="学习算法">学习算法</h2>
<p>1） ID3算法：处理离散值的量</p>
<p>2） C45算法：处理连续值的量</p>
<p>3）Cart算法：离散和连续 两者都合适？</p>
<p>关于CART：Cart可以通过特征的选择迭代建立一颗分类树，使得每次的分类平面能最好的将剩余数据分为两类 gini=1-sigma(pi^2)，表示每个类别出现的概率和与1的差值。分类问题：argmax（Gini-GiniLeft-GiniRight），回归问题argmax(Var-VarLeft-VarRight)，查找最佳特征f已经最佳属性阈值th 小于th的在左边，大于th的在右边子树。</p>
<h2 id="随机森林特点">随机森林特点</h2>
<p>1）能够处理大量特征的分类，并且还不用做特征选择</p>
<p>2）在训练完成之后能给出哪些feature的比较重要</p>
<p>3）训练速度很快</p>
<p>4）很容易并行</p>
<p>5）实现相对来说较为简单</p>
<h2 id="具体学习过程">具体学习过程</h2>
<h3 id="训练过程">训练过程</h3>
<ol>
<li>
<pre><code>给定训练集S，测试集T，特征维数F。
</code></pre>
</li>
<li>
<pre><code>确定参数：使用到的CART的数量为t，每棵树的深度d，每个节点使用到的特征数量f，终止条件：节点上最少样本数s，节点上最少的信息增益m。
</code></pre>
</li>
<li>
<pre><code>对于第t-1棵树，i=t-1: 从S中有放回的抽取大小和S一样的训练集S(i)，作为根节点的样本，从根节点开始训练（随机样本）。
</code></pre>
</li>
<li>
<pre><code>如果当前节点上达到终止条件，则设置当前节点为叶子节点，如果是分类问题，该叶子节点的预测输出为当前节点样本集合中数量最多的哪一类c(j)，概率p为c(j)占当前样本集的比例；如果是回归问题，预测输出为当前节点样本集各个样本值的平均值。然后继续训练其他节点。
</code></pre>
</li>
<li>
<pre><code>如果当前节点没有达到终止条件，则从F维特征中无放回的随机选取f维度特征。利用这f维特征，寻找分类效果最好的一维特征k及其阈值th，当前节点上样本第k维特征小于th的样本被划分到左节点，其余被划分到右节点。继续训练其他节点（随机特征）。
</code></pre>
</li>
<li>
<pre><code>重复(3)(4) (5)直到所有节点都训练过了或者被标记为叶子节点。
</code></pre>
</li>
<li>
<pre><code>重复(3)(4) (5)直到所有CART都被训练过。
</code></pre>
</li>
</ol>
<h3 id="预测过程-2">预测过程</h3>
<p>对于第t-1棵树，i=t-1;</p>
<ol>
<li>
<pre><code>从当前树的根节点开始，根据当前节点的阈值th，判断是进入左节点(&lt;th)还是进入右节点(&gt;=th)，直到到达某个叶子节点，并输出预测值。
</code></pre>
</li>
<li>
<pre><code>重复执行(1)直到所有t棵树都输出了预测值。如果是分类问题，则输出为所有树种预测概率总和最大的那一个类，即对每个c(j)的p进行累计；如果是回归问题，则输出为所有树的输出的平均值。
</code></pre>
</li>
</ol>
<p>影响分类性能的主要因素</p>
<ol>
<li>
<pre><code>森林中单棵树的分类强度：每棵树的分类强度越大，则随机森林的分类性能越好。
</code></pre>
</li>
<li>
<pre><code>森林中树之间的相关度：树之间的相关度越大，则随机森林的分类性能越差。
</code></pre>
</li>
</ol>
<h2 id="随机森林的优缺点">随机森林的优缺点</h2>
<p>随机森林，指的是利用多棵树对样本进行训练并预测的一种分类器。</p>
<p>①样本随机性：对于每棵树，它们使用的训练集是从总的训练集中有放回采样出来的。这意味着，总的训练集中的有些样本可能多次出现在一棵树的训练集中，也可能从未出现在一棵树的训练集中。</p>
<p>②特征随机性：在训练每棵树的节点时，使用的特征是从所有特征中按照一定比例随机地无放回抽取的，根据Leo Breiman的建议，假设总的特征数量为M，这个比例可以是sqrt(M)，1/2sqrt(M)，2sqrt(M)。</p>
<p>优点：</p>
<ol>
<li>
<pre><code>两个随机性的引入，使得随机森林不容易陷入过拟合，并且具有很好的抗噪能力。
</code></pre>
</li>
<li>
<pre><code>它能够处理很高维度的数据，并且不用做特征选择，由于模型可以自动得出变量重要性排序（基于OOB误分率的增加量和基于分裂时的GINI下降量）。
</code></pre>
</li>
<li>
<pre><code>对数据集的适应能力强：既能处理离散型数据，也能处理连续型数据，数据集无需规范化。
</code></pre>
</li>
<li>
<pre><code>训练速度快，训练时树与树之间是相互独立的，容易做成并行化方法。
</code></pre>
</li>
<li>
<pre><code>在训练过程中，能够检测到feature间的互相影响。可生成一个Proximities=(pij)矩阵，用于度量样本之间的相似性：pij=aij/N，aji表示样本i和j出现在随机森林中同一个叶子结点的次数，N表示随机森林中树的棵树。
</code></pre>
</li>
<li>
<pre><code>对于不平衡的数据集来说，它可以平衡误差。
</code></pre>
</li>
<li>
<pre><code>如果有很大一部分的特征遗失，仍可以维持准确度。
</code></pre>
</li>
</ol>
<p>缺点：</p>
<ol>
<li>
<pre><code>随机森林已经被证明在某些噪音较大的分类或回归问题上会过拟。
</code></pre>
</li>
<li>
<pre><code>对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的。
</code></pre>
</li>
</ol>
<h2 id="随机森林的特征选择方法">随机森林的特征选择方法</h2>
<p>1）平均不纯度减少（mean decrease impurity）</p>
<p>利用不纯度可以确定划分的最优节点。对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。当训练决策树的时候，可以计算出每个特征减少了多少树的不纯度。对于一个决策树森林来说，可以算出每个特征平均减少了多少不纯度，并把它平均减少的不纯度作为特征选择的值。</p>
<p>值得注意：1、这种方法存在偏向，对具有更多类别的变量会更有利（ID3）；2、对于存在关联的多个特征，其中任意一个都可以作为指示器（优秀的特征），并且一旦某个特征被选择之后，其他特征的重要度就会急剧下降，因为不纯度已经被选中的那个特征降下来了，其他的特征就很难再降低那么多不纯度了，这样一来，只有先被选中的那个特征重要度很高，其他的关联特征重要度往往较低。在理解数据时，这就会造成误解，导致错误的认为先被选中的特征是很重要的，而其余的特征是不重要的，但实际上这些特征对响应变量的作用确实非常接近的。</p>
<p>2）平均精确率减少（Mean decrease accuracy）</p>
<p>直接度量每个特征对模型精确率的影响。主要思路是打乱每个特征的特征值顺序（也即打乱划分节点的优先级顺序），并且度量顺序变动对模型的精确率的影响。很明显，对于不重要的变量来说，打乱顺序对模型的精确率影响不会太大，但是对于重要的变量来说，打乱顺序就会降低模型的精确率。</p>
<p><strong>鸣谢</strong></p>
<p><a href="http://blog.csdn.net/keepreder/article/details/47277517">http://blog.csdn.net/keepreder/article/details/47277517</a></p>
<p><a href="http://blog.csdn.net/u012102306/article/details/52228516">http://blog.csdn.net/u012102306/article/details/52228516</a></p>
<p><a href="https://www.cnblogs.com/amberdata/p/7203632.html">https://www.cnblogs.com/amberdata/p/7203632.html</a></p>

          </div>
        </div>

        
          <div class="next-post">
            <a class="purple-link" href="https://aprilspring.github.io/post/jue-ce-shu/">
              <h3 class="post-title">
                下一篇：决策树
              </h3>
            </a>
          </div>
          
      </div>

      
        
          <div id="gitalk-container"></div>
        

        
      

      <div class="site-footer">
  <div class="slogan">温故而知新</div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | <a class="rss" href="https://aprilspring.github.io/atom.xml" target="_blank">RSS</a>
</div>


    </div>
    <script type="application/javascript">

hljs.initHighlightingOnLoad()

var app = new Vue({
  el: '#app',
  data: {
    menuVisible: false,
  },
})

</script>



  
    <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
    <script>

      var gitalk = new Gitalk({
        clientID: '337e2992c146020878cb',
        clientSecret: '883fa6db7cdfafda50356191ec9938e4d661aca1',
        repo: 'AprilSpring.github.io',
        owner: 'AprilSpring',
        admin: ['AprilSpring'],
        id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
        distractionFreeMode: false  // Facebook-like distraction free mode
      })

      gitalk.render('gitalk-container')

    </script>
  

  




  </body>
</html>
